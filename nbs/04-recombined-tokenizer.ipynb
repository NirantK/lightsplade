{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, Iterable\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_sparse_tools import convert_sparse_vector\n",
    "from remap_tokens import (\n",
    "    aggregate_weights,\n",
    "    filter_pair_tokens,\n",
    "    reconstruct_bpe,\n",
    "    # rescore_vector,\n",
    "    stem_pair_tokens,\n",
    ")\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "dataset_name = \"scifact\"\n",
    "source_model = \"nirantk/splade-v3-lexical\"\n",
    "source_col_name = \"spalde-v3-lexical\"\n",
    "col_name = \"splade-snowball\"\n",
    "collection_name = f\"{dataset_name}-{col_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(f\"nirantk/{dataset_name}-sparse-vectors\", split=\"train\")\n",
    "ds[source_col_name][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sparse_vectors = [json.loads(x) for x in ds[source_col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_pretrained(source_model)\n",
    "reverse_voc = {v: k for k, v in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vectors = []\n",
    "for sv in source_sparse_vectors:\n",
    "    raw_vectors.append(\n",
    "        {\n",
    "            \"tokens\": [reverse_voc[int(key)] for key in sv.keys()],\n",
    "            \"weights\": list(sv.values()),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recombine and Retokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_weight = {}\n",
    "num_tokens = {}\n",
    "\n",
    "total_tokens = 0\n",
    "\n",
    "for sentence in raw_vectors:\n",
    "    print(\"tokens:\\t\", sentence['tokens'])\n",
    "\n",
    "    reconstructed = reconstruct_bpe(enumerate(sentence[\"tokens\"]))\n",
    "\n",
    "    print(\"reconstructed:\\t\", reconstructed)\n",
    "\n",
    "    filtered_reconstructed = filter_pair_tokens(reconstructed)\n",
    "\n",
    "    print(\"filtered:\\t\", filtered_reconstructed)\n",
    " \n",
    "    stemmed_reconstructed = stem_pair_tokens(filtered_reconstructed)\n",
    "\n",
    "    print(\"stemmed:\\t\", stemmed_reconstructed)\n",
    "\n",
    "    weighed_reconstructed = aggregate_weights(\n",
    "        stemmed_reconstructed, sentence[\"weights\"]\n",
    "    )\n",
    "\n",
    "    print(\"weighed:\\t\", weighed_reconstructed)\n",
    "\n",
    "    total_tokens += len(weighed_reconstructed)\n",
    "\n",
    "    for reconstructed_token, score in weighed_reconstructed:\n",
    "        max_token_weight[reconstructed_token] = max(\n",
    "            max_token_weight.get(reconstructed_token, 0), score\n",
    "        )\n",
    "        num_tokens[reconstructed_token] = num_tokens.get(reconstructed_token, 0) + 1\n",
    "\n",
    "    print()\n",
    "    break\n",
    "\n",
    "# tokens = stem_list_tokens(filter_list_tokens(snowball_tokenize(text)))\n",
    "# total_tokens = len(tokens)\n",
    "# num_tokens = Counter(tokens)\n",
    "\n",
    "sparse_vector = {}\n",
    "\n",
    "token_score = rescore_vector(max_token_weight)\n",
    "\n",
    "for token, token_count in num_tokens.items():\n",
    "    score = token_score[token]\n",
    "    tf = score + token_count - 1\n",
    "    # tf = token_count\n",
    "    sparse_vector[token] = calc_tf(tf, total_tokens)\n",
    "\n",
    "out_file.write(json.dumps(sparse_vector) + \"\\n\")\n",
    "\n",
    "total_tokens_overall += total_tokens\n",
    "num_docs += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(url=os.getenv(\"QDRANT_URL\"), api_key=os.getenv(\"QDRANT_API_KEY\"))\n",
    "\n",
    "\n",
    "def is_empty(client: QdrantClient, collection_name: str) -> bool:\n",
    "    return client.get_collection(collection_name).points_count == 0\n",
    "\n",
    "\n",
    "# client.delete_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not client.collection_exists(collection_name):\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config={},\n",
    "        sparse_vectors_config={\n",
    "            \"splade\": models.SparseVectorParams(\n",
    "                index=models.SparseIndexParams(on_disk=False)\n",
    "            )\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataset_name: str) -> Iterable[models.PointStruct]:\n",
    "    ds = load_dataset(f\"nirantk/{dataset_name}-sparse-vectors\", split=\"train\")\n",
    "    print(\"Columns: \", ds.features)\n",
    "    ds = ds.to_list()\n",
    "    for element in ds:\n",
    "        yield models.PointStruct(\n",
    "            id=int(element[\"_id\"]),\n",
    "            vector={\"splade\": convert_sparse_vector(json.loads(element[col_name]))},\n",
    "            payload={\n",
    "                \"text\": element[\"text\"],\n",
    "                \"title\": element[\"title\"],\n",
    "                \"id\": element[\"_id\"],\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "# Run ONCE to upload data, only when collection is empty\n",
    "if is_empty(client, collection_name):\n",
    "    client.upload_points(\n",
    "        collection_name=collection_name, points=tqdm(read_data(dataset_name))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightsplade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
