{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, Iterable\n",
    "\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient, models\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "dataset_name = \"scifact\"\n",
    "col_name = \"spalde-v3-lexical\"\n",
    "collection_name = f\"{dataset_name}-{col_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='scifact-spalde-v3-lexical')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = QdrantClient(url=os.getenv(\"QDRANT_URL\"), api_key=os.getenv(\"QDRANT_API_KEY\"))\n",
    "\n",
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/grpbcmrd36gc7q5_11whbn540000gn/T/ipykernel_83180/3179846115.py:1: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config={},\n",
    "    sparse_vectors_config={\n",
    "        \"splade\": models.SparseVectorParams(\n",
    "            index=models.SparseIndexParams(on_disk=False)\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_vector(sparse_vector: Dict) -> models.SparseVector:\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for idx, value in sparse_vector.items():\n",
    "        indices.append(int(idx))\n",
    "        values.append(value)\n",
    "\n",
    "    return models.SparseVector(indices=indices, values=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a60d80b9b3a453caccce747b9f3e42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  {'spalde-v3-lexical': Value(dtype='string', id=None), '_id': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "def read_data(dataset_name: str) -> Iterable[models.PointStruct]:\n",
    "    ds = load_dataset(f\"nirantk/{dataset_name}-sparse-vectors\", split=\"train\")\n",
    "    print(\"Columns: \", ds.features)\n",
    "    ds = ds.to_list()\n",
    "    for element in ds:\n",
    "        yield models.PointStruct(\n",
    "            id=int(element[\"_id\"]),\n",
    "            vector={\"splade\": convert_sparse_vector(json.loads(element[col_name]))},\n",
    "            payload={\n",
    "                \"text\": element[\"text\"],\n",
    "                \"title\": element[\"title\"],\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "client.upload_points(\n",
    "    collection_name=collection_name, points=tqdm(read_data(dataset_name))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_dataset(f\"BeIR/{dataset_name}\", \"queries\")[\"queries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dfe16d14d54fd2acefb8b1827d6f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(\"nirantk/splade-v3-lexical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=4, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextInputSequence must be str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: TextInputSequence must be str"
     ]
    }
   ],
   "source": [
    "tokenizer.encode(queries[\"text\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightsplade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
