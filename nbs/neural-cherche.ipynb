{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_cherche import models, utils, train, losses\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "from time import time\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "model_name = \"raphaelsty/neural-cherche-sparse-embed\"\n",
    "\n",
    "# Check that MPS is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SparseEmbed(\n",
    "    model_name_or_path=model_name,\n",
    "    device=\"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-6)\n",
    "flops_scheduler = losses.FlopsScheduler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_to_tuple_input(triple: dict):\n",
    "    keys = triple['metadata']['objective']['triplet'][0]\n",
    "    return (\n",
    "        triple[keys[0]],\n",
    "        triple[keys[1]],\n",
    "        triple[keys[2]],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.ls = lambda x: list(x.iterdir())\n",
    "nli_triplets = Path(\"../nomic-triplets/nli-triplets\")\n",
    "nli_triplets.ls()\n",
    "\n",
    "triplets = []\n",
    "for file in nli_triplets.ls():\n",
    "    if \"shard\" in file.name:\n",
    "        with file.open(\"r\") as f:\n",
    "            file_content = f.readlines()\n",
    "            triplets.extend([triplet_to_tuple_input(json.loads(line)) for line in file_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = Dataset.from_list(triplets)\n",
    "# ds.push_to_hub(\"NirantK/nli-triplets\", token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def display_tokens_and_weights(sparse_embedding, tokenizer):\n",
    "    token_weight_dict = {}\n",
    "    for i in range(len(sparse_embedding.indices)):\n",
    "        token = tokenizer.decode([sparse_embedding.indices[i]])\n",
    "        weight = sparse_embedding.values[i]\n",
    "        token_weight_dict[token] = weight\n",
    "\n",
    "    # Sort the dictionary by weights\n",
    "    token_weight_dict = dict(sorted(token_weight_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    print(json.dumps(token_weight_dict, indent=4))\n",
    "    return token_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 62/17321 [03:56<18:15:10,  3.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (anchor, positive, negative) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(utils\u001b[38;5;241m.\u001b[39miter(\n\u001b[1;32m      2\u001b[0m         triplets,\n\u001b[1;32m      3\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      4\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m      5\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     )):\n\u001b[0;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sparse_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43manchor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manchor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreshold_flops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflops_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflops_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# Save the model every 1000 steps\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/lightsplade/lib/python3.10/site-packages/neural_cherche/train/train_sparse_embed.py:157\u001b[0m, in \u001b[0;36mtrain_sparse_embed\u001b[0;34m(model, optimizer, anchor, positive, negative, flops_loss_weight, sparse_loss_weight, dense_loss_weight, in_batch_negatives, threshold_flops, max_flops_loss, step, gradient_accumulation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m loss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    151\u001b[0m     dense_loss_weight \u001b[38;5;241m*\u001b[39m dense_ranking_loss\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;241m+\u001b[39m sparse_loss_weight \u001b[38;5;241m*\u001b[39m sparse_ranking_loss\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;241m+\u001b[39m flops_loss_weight \u001b[38;5;241m*\u001b[39m flops_loss\n\u001b[1;32m    154\u001b[0m )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m gradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    160\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/lightsplade/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/lightsplade/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for step, (anchor, positive, negative) in enumerate(utils.iter(\n",
    "        triplets,\n",
    "        epochs=2,\n",
    "        batch_size=16,\n",
    "        shuffle=True\n",
    "    )):\n",
    "\n",
    "    loss = train.train_sparse_embed(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        anchor=anchor,\n",
    "        positive=positive,\n",
    "        negative=negative,\n",
    "        threshold_flops=30,\n",
    "        flops_loss_weight=flops_scheduler.get(),\n",
    "        step=step,\n",
    "        gradient_accumulation_steps=50,\n",
    "    )\n",
    "\n",
    "    if (step + 1) % 50 == 0:\n",
    "        # Save the model every 1000 steps\n",
    "        model.save_pretrained(\"checkpoint\")\n",
    "        checkpoint = models.SparseEmbed(\n",
    "            model_name_or_path=\"checkpoint\",\n",
    "            device=\"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "        )\n",
    "        time_now = time()\n",
    "        query_activations = checkpoint.encode([\"Hello World from Qdrant!\"], query_mode=True)\n",
    "        encode_time = time() - time_now\n",
    "        print(f\"Query Encode time: {encode_time}\")\n",
    "        print(tokenizer.decode(query_activations['activations'][0]))\n",
    "        time_now = time()\n",
    "        document_activations = checkpoint.encode([\"Hello World from Qdrant!\"], query_mode=False)\n",
    "        encode_time = time() - time_now\n",
    "        print(f\"Document Encode time: {encode_time}\")\n",
    "        print(tokenizer.decode(document_activations['activations'][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightsplade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
