{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import Dict, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_sparse_tools import convert_sparse_vector\n",
    "from retokenize import (\n",
    "    aggregate_weights,\n",
    "    process_text,\n",
    "    reconstruct_bpe,\n",
    "    stem_words,\n",
    ")\n",
    "\n",
    "from loguru import logger\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "canonical_dataset_name = \"scifact\"\n",
    "dataset_name = \"scifact-bge-m3-sparse-vectors\"\n",
    "col_name = \"bge_m3_sparse_vector\"\n",
    "collection_name = f\"{dataset_name}-{col_name}-retok\"\n",
    "model_name = \"BAAI/bge-m3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(f\"nirantk/{dataset_name}\", split=\"corpus\")\n",
    "ds[col_name][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_vectors = [json.loads(x) for x in ds[col_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change schema to Qdrant Sparse Vector\n",
    "\n",
    "1. Create vocab and reverse vocab from Tokenizer corresponding to the model\n",
    "2. Invert the integer index to token string\n",
    "3. Make lists from the keys and values of the vocab dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "vocab = tokenizer.get_vocab()\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vectors = []\n",
    "for sv in sparse_vectors:\n",
    "    raw_vectors.append(\n",
    "        {\n",
    "            \"tokens\": [reverse_vocab[int(key)] for key in sv.keys()],\n",
    "            \"weights\": list(sv.values()),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"text\"][0], raw_vectors[0][\"tokens\"][0:10], raw_vectors[0][\"weights\"][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recombine and Retokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescored_vectors = []\n",
    "# logger.level(\"DEBUG\")\n",
    "logger.level(\"INFO\")\n",
    "for source_sparse_vector in tqdm(raw_vectors):\n",
    "    reconstructed_words = reconstruct_bpe(source_sparse_vector[\"tokens\"])\n",
    "    logger.debug(f\"Reconstructed words: {reconstructed_words}\")\n",
    "    stemmed_words = stem_words(reconstructed_words)\n",
    "    logger.debug(f\"Stemmed words: {stemmed_words}\")\n",
    "    # reweighted_sparse_vector = process_text(\n",
    "    #     bpe_tokens=source_sparse_vector[\"tokens\"],\n",
    "    #     weights=source_sparse_vector[\"weights\"],\n",
    "    #     aggregate_fn=aggregate_weights,\n",
    "    # )\n",
    "    # rescored_vectors.append(reweighted_sparse_vector)\n",
    "\n",
    "# reweighted_sparse_vectors = []\n",
    "# for source_sparse_vector, text in tqdm(\n",
    "#     zip(raw_vectors, ds[\"text\"]), total=len(raw_vectors)\n",
    "# ):\n",
    "#     reweighted_sparse_vector = process_text(\n",
    "#         bpe_tokens=source_sparse_vector[\"tokens\"],\n",
    "#         weights=source_sparse_vector[\"weights\"],\n",
    "#         aggregate_fn=aggregate_weights,\n",
    "#     )\n",
    "#     # print(len(source_sparse_vectors))\n",
    "#     reweighted_sparse_vectors.append(reweighted_sparse_vector)\n",
    "#     # print(len(reweighted_sparse_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find length of each sparse vector\n",
    "vector_lengths = [len(sv) for sv in reweighted_sparse_vectors]\n",
    "\n",
    "# Percentile of the lengths\n",
    "np.percentile(vector_lengths, [10, 50, 90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(reweighted_sparse_vectors), reweighted_sparse_vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(url=os.getenv(\"QDRANT_URL\"), api_key=os.getenv(\"QDRANT_API_KEY\"))\n",
    "client.collectio\n",
    "\n",
    "\n",
    "def is_empty(client: QdrantClient, collection_name: str) -> bool:\n",
    "    return client.get_collection(collection_name).points_count == 0\n",
    "\n",
    "\n",
    "# client.delete_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_collection(client: QdrantClient, collection_name: str):\n",
    "    if client.collection_exists(collection_name):\n",
    "        client.delete_collection(collection_name)\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config={},\n",
    "        sparse_vectors_config={\n",
    "            col_name: models.SparseVectorParams(\n",
    "                index=models.SparseIndexParams(on_disk=False)\n",
    "            )\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a vocab of all keys in the reweighted sparse vectors\n",
    "vocab = set()\n",
    "for sv in reweighted_sparse_vectors:\n",
    "    vocab.update(sv.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this into a vocab object with each string having an id\n",
    "vocab = {word: i for i, word in enumerate(vocab)}\n",
    "invert_vocab = {i: word for word, i in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute the reweighted sparse vectors with the new vocab\n",
    "id_reweighted_sparse_vectors = []\n",
    "for sv in tqdm(reweighted_sparse_vectors):\n",
    "    new_sv = {}\n",
    "    for word, weight in sv.items():\n",
    "        new_sv[vocab[word]] = weight\n",
    "    id_reweighted_sparse_vectors.append(new_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable: Iterable, n: int = 1) -> Iterable:\n",
    "    \"\"\"Yield successive n-sized chunks from iterable.\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i : i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_points(\n",
    "    reweighted_sparse_vectors: Dict, ds: Dataset\n",
    ") -> Iterable[models.PointStruct]:\n",
    "    points = []\n",
    "    for sv, element in tqdm(zip(reweighted_sparse_vectors, ds)):\n",
    "        points.append(\n",
    "            models.PointStruct(\n",
    "                id=int(element[\"_id\"]),\n",
    "                vector={col_name: convert_sparse_vector(sv)},\n",
    "                payload={\n",
    "                    \"text\": element[\"text\"],\n",
    "                    \"title\": element[\"title\"],\n",
    "                    \"id\": element[\"_id\"],\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "    return points\n",
    "\n",
    "\n",
    "# next(read_data(id_reweighted_sparse_vectors, ds))\n",
    "reset_collection(client, collection_name)\n",
    "points = make_points(id_reweighted_sparse_vectors, ds)\n",
    "# Run ONCE to upload data, only when collection is empty\n",
    "for batch in tqdm(batched(points, 100)):\n",
    "    try:\n",
    "        client.upload_points(collection_name=collection_name, points=batch)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(f\"../data/{dataset_name}/qrels/test.tsv\", sep=\"\\t\")\n",
    "test[\"query-id\"] = test[\"query-id\"].astype(int)\n",
    "\n",
    "with open(f\"../data/{dataset_name}/queries.jsonl\") as f:\n",
    "    queries = [json.loads(line) for line in f]\n",
    "\n",
    "# Only keep the test set queries\n",
    "queries = [q for q in queries if int(q[\"_id\"]) in list(test[\"query-id\"])]\n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_pretrained(\"nirantk/splade-v3-lexical\")\n",
    "tokens = [tokenizer.encode(q[\"text\"]).tokens for q in queries]\n",
    "tokens = [list(set(t)) for t in tokens]\n",
    "# tokens = [list(set(t.ids)) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50\n",
    "tokens[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign weight to all tokens and create a query vector with tokens and weights as keys\n",
    "query_vectors = []\n",
    "for token in tokens:\n",
    "    query_vector = {}\n",
    "    query_vector[\"tokens\"] = token\n",
    "    query_vector[\"weights\"] = [1] * len(token)\n",
    "    query_vectors.append(query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retokenize all the query tokens\n",
    "reweighted_query_tokens = []\n",
    "for qv, text in tqdm(zip(query_vectors, [q[\"text\"] for q in queries])):\n",
    "    # print(text)\n",
    "    # print(qv)\n",
    "    reweighted_query_tokens.append(\n",
    "        retokenize_sparse_vector(\n",
    "            source_sparse_vector=qv, text=text, tokenizer=tokenizer\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reweighted_query_tokens[idx + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile([len(t) for t in reweighted_query_tokens], [10, 50, 90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the keys back to the original vocab with integer ids\n",
    "id_reweighted_query_tokens = []\n",
    "for qv in tqdm(reweighted_query_tokens):\n",
    "    new_qv = {}\n",
    "    for word, weight in qv.items():\n",
    "        try:\n",
    "            new_qv[vocab[word]] = weight\n",
    "        except KeyError:\n",
    "            print(word)\n",
    "            continue\n",
    "    id_reweighted_query_tokens.append(new_qv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_query_vectors = [\n",
    "    models.SparseVector(\n",
    "        indices=qv.keys(),\n",
    "        values=qv.values(),\n",
    "    )\n",
    "    for qv in id_reweighted_query_tokens\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_query_vectors[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 10\n",
    "results = []\n",
    "for qv in tqdm(qdrant_query_vectors):\n",
    "    try:\n",
    "        result = client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=models.NamedSparseVector(name=col_name, vector=qv),\n",
    "            with_payload=True,\n",
    "            limit=limit,\n",
    "        )\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(qv)\n",
    "        results.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids, doc_ids, ranks, scores = [], [], [], []\n",
    "for query, result in zip(queries, results):\n",
    "    query_id = query[\"_id\"]\n",
    "    result_ids = [str(r.id) for r in result]\n",
    "    result_scores = [r.score for r in result]\n",
    "    result_ranks = list(range(len(result)))\n",
    "    query_ids.extend(len(result) * [query_id])\n",
    "    doc_ids.extend(result_ids)\n",
    "    ranks.extend(result_ranks)\n",
    "    scores.extend(result_scores)\n",
    "    # print(f\"query: {query_id}\")\n",
    "    # print(f\"docid: {result_ids}\")\n",
    "    # print(f\"rank: {result_ranks}\")\n",
    "    # print(f\"score: {result_scores}\")\n",
    "\n",
    "run = {\n",
    "    \"query\": [int(q) for q in query_ids],\n",
    "    \"q0\": len(query_ids) * [\"q0\"],\n",
    "    \"docid\": doc_ids,\n",
    "    \"rank\": ranks,\n",
    "    \"score\": scores,\n",
    "    \"system\": len(query_ids) * [\"splade\"],\n",
    "}\n",
    "\n",
    "with open(\"lexical-retokenize-rescore.run.json\", \"w\") as f:\n",
    "    json.dump(run, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightsplade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
